# -*- coding: utf-8 -*-
"""Orchestrates query processing using vector search and LLM.

This module takes a user query, retrieves relevant context using vector search,
and then uses an LLM to generate an answer based on the query and context.
"""

import logging
from typing import Optional, List, Dict, Any

# Assuming these modules are in the same src directory or PYTHONPATH is set up correctly
from src.vectorizer import vector_store_handler
from src.extractor import llm_client

# Import Langchain message types
try:
    from langchain_core.messages import HumanMessage, SystemMessage
except ImportError:
    # Handle potential ImportError if langchain_core is not available
    # This is more for robustness; in a controlled environment, it should be installed.
    logging.critical("langchain_core.messages not found. Please ensure langchain-core is installed.")
    # Fallback to basic dicts or raise an error, depending on desired handling
    # For this implementation, we'll assume it's available. If not, errors will occur at runtime.
    pass


logger = logging.getLogger(__name__)

DEFAULT_TOP_K_CONTEXT_CHUNKS = 3

async def get_llm_answer_with_context(
    user_query: str,
    document_filename: Optional[str] = None,
    top_k_context_chunks: int = DEFAULT_TOP_K_CONTEXT_CHUNKS
) -> Dict[str, Any]:
    """
    Processes a user query by retrieving relevant context from stored documents
    and then using an LLM to generate an answer based on the query and context.

    Args:
        user_query: The question asked by the user.
        document_filename: Optional. If provided, context search is restricted
                           to chunks from this specific document.
        top_k_context_chunks: The number of most relevant text chunks to retrieve
                              as context for the LLM. Defaults to `DEFAULT_TOP_K_CONTEXT_CHUNKS`.

    Returns:
        A dictionary containing:
            - "answer" (str): The answer generated by the LLM. Could be an error message if issues occur.
            - "retrieved_context" (List[Dict[str, Any]]): The list of context chunks
              used for generating the answer (can be empty).
            - "error" (Optional[str]): An error message string if any part of the process failed significantly,
              otherwise None.
    """
    logger.info(f"Processing query: '{user_query[:100]}...' with filename filter: '{document_filename}', top_k_context: {top_k_context_chunks}")

    retrieved_chunks: List[Dict[str, Any]] = []
    error_message: Optional[str] = None
    answer: str = "Não foi possível obter uma resposta devido a um erro interno." # Default error answer

    try:
        # 1. Retrieve Context
        logger.info("Buscando chunks de contexto relevantes...")
        retrieved_chunks = await vector_store_handler.search_similar_chunks(
            query_text=user_query,
            top_k=top_k_context_chunks,
            document_filename=document_filename
        )

        if not retrieved_chunks:
            logger.info(f"Nenhum chunk de contexto encontrado para a query: '{user_query[:100]}...' com filtro: {document_filename}")
            # Proceed with empty context, LLM should indicate it cannot answer based on context.
        else:
            logger.info(f"Recuperados {len(retrieved_chunks)} chunks de contexto.")

        # 2. Format Context String
        context_parts = [chunk.get('text_content', '') for chunk in retrieved_chunks if chunk.get('text_content')]
        context_string = "\n\n---\n\n".join(context_parts)

        if not context_string and retrieved_chunks: # Chunks were found but all had empty text_content
             logger.warning("Context string is empty even after retrieving chunks (e.g., all text_content was empty).")
        elif not context_string: # No chunks found or they were empty
             logger.info("Nenhum conteúdo de contexto para enviar ao LLM.")


        # 3. Construct Prompt Messages
        system_prompt_text = (
            "Você é um assistente de IA prestativo que responde a perguntas concisamente com base no contexto fornecido. "
            "Se o contexto não contiver informações suficientes para responder à pergunta, "
            "afirme que você não pode responder com base nas informações fornecidas. "
            "Responda sempre em Português."
        )
        system_message = SystemMessage(content=system_prompt_text)

        human_prompt_template = (
            "Contexto Fornecido:\n"
            "---------------------\n"
            "{context_string}\n"
            "---------------------\n"
            "Com base no contexto acima, responda à seguinte pergunta de forma concisa e direta:\n"
            "Pergunta: {user_query}\n\n"
            "Resposta Concisa:"
        )
        human_prompt_text = human_prompt_template.format(context_string=context_string if context_string else "Nenhum contexto fornecido.", user_query=user_query)
        human_message = HumanMessage(content=human_prompt_text)

        messages = [system_message, human_message]

        # 4. Call LLM
        logger.info("Obtendo cliente LLM...")
        llm = llm_client.get_llm_client() # This client is synchronous

        logger.info("Enviando prompt para o LLM...")
        # Langchain's BaseChatModel.invoke is synchronous
        llm_response = llm.invoke(messages)

        if hasattr(llm_response, 'content'):
            answer = llm_response.content
            logger.info("Resposta recebida do LLM.")
        else:
            logger.error(f"Resposta do LLM não possui atributo 'content'. Resposta recebida: {llm_response}")
            answer = "Formato de resposta inesperado do LLM."
            error_message = "Formato de resposta inesperado do LLM."


    except ConnectionError as ce: # Raised by vector_store_handler for DB issues
        logger.error(f"Erro de conexão ao buscar chunks de contexto: {ce}", exc_info=True)
        error_message = f"Erro de conexão com o banco de dados ao buscar contexto: {str(ce)}"
        answer = "Erro ao buscar contexto no banco de dados. Por favor, tente novamente mais tarde."
    except llm_client.ValueError as llm_val_err: # Specific ValueError from llm_client (e.g. API key)
        logger.error(f"Erro de configuração do cliente LLM: {llm_val_err}", exc_info=True)
        error_message = f"Erro de configuração do LLM: {str(llm_val_err)}"
        answer = "Erro na configuração do serviço de linguagem. Verifique as credenciais."
    except Exception as e: # Catch any other unexpected errors
        logger.error(f"Erro inesperado durante o processamento da query: {e}", exc_info=True)
        error_message = f"Erro inesperado: {str(e)}"
        answer = "Ocorreu um erro inesperado ao processar sua pergunta. Por favor, tente novamente mais tarde."

    return {
        "answer": answer,
        "retrieved_context": retrieved_chunks, # Return chunks with their full details
        "error": error_message
    }

# Example of how it might be called (for testing or direct use later):
# if __name__ == '__main__':
#     import asyncio
#     async def main_test(): # Renamed to avoid conflict
#         # This would require the .env to be set up for DB and LLM access
#         # and some data to be present in the vector store.
#         logging.basicConfig(
#             level=logging.INFO,
#             format='%(asctime)s - %(levelname)s - %(name)s - %(module)s - %(funcName)s - %(message)s'
#         )
#
#         # Ensure DB and vector store are populated for a meaningful test.
#         # Example: Add some chunks first (simplified, direct call for testing)
#         # try:
#         #     from src.vectorizer.embedding_generator import generate_embeddings_for_chunks as embed_func
#         #     sample_chunks_for_db = [
#         #         {"chunk_id": "docX_chunk1_test_qp", "text_content": "O céu é azul porque reflete o oceano.", "metadata": {"filename": "docX.pdf", "page_number": 1}},
#         #         {"chunk_id": "docX_chunk2_test_qp", "text_content": "A grama é verde devido à clorofila.", "metadata": {"filename": "docX.pdf", "page_number": 1}},
#         #         {"chunk_id": "docY_chunk1_test_qp", "text_content": "Inteligência artificial é um campo vasto.", "metadata": {"filename": "docY.pdf", "page_number": 1}},
#         #     ]
#         #     chunks_with_embeds = embed_func(sample_chunks_for_db) # Sync call
#         #     await vector_store_handler.add_chunks_to_vector_store(chunks_with_embeds)
#         #     logger.info("Amostra de chunks inserida/atualizada para teste.")
#         # except Exception as db_init_e:
#         #     logger.error(f"Falha ao inicializar DB com dados de teste: {db_init_e}")
#         #     return
#
#         test_query = "Por que o céu é azul?"
#         test_filename_filter = "docX.pdf"
#
#         logger.info(f"--- Testando query: '{test_query}' com filtro: '{test_filename_filter}' ---")
#         response = await get_llm_answer_with_context(test_query, document_filename=test_filename_filter)
#
#         logger.info(f"\n--- RESULTADO FINAL ---")
#         logger.info(f"Query: {test_query}")
#         logger.info(f"LLM Answer: {response.get('answer')}")
#         if response.get('error'):
#             logger.error(f"Error: {response.get('error')}")
#
#         logger.info(f"Retrieved context snippets: {len(response.get('retrieved_context', []))} chunk(s)")
#         for i, chunk_info in enumerate(response.get('retrieved_context', [])):
#             score_str = f"{chunk_info.get('similarity_score'):.4f}" if chunk_info.get('similarity_score') is not None else "N/A"
#             logger.info(f"  Context Chunk {i+1} (ID: {chunk_info.get('chunk_id')}, Score: {score_str}):")
#             logger.info(f"    '{chunk_info.get('text_content', '')[:150]}...'\n")
#
#     asyncio.run(main_test())
